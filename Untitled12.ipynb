{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2cee6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 120469 entries, 0 to 120491\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   order_id          120469 non-null  object \n",
      " 1   timestamp         120469 non-null  float64\n",
      " 2   user_id           120469 non-null  object \n",
      " 3   product_id        120469 non-null  object \n",
      " 4   quantity          120469 non-null  float64\n",
      " 5   user_rating       120469 non-null  float64\n",
      " 6   price             120469 non-null  float64\n",
      " 7   user_gender       120469 non-null  object \n",
      " 8   user_age          120469 non-null  float64\n",
      " 9   user_lat          120469 non-null  float64\n",
      " 10  user_long         120469 non-null  float64\n",
      " 11  product_colour    120469 non-null  object \n",
      " 12  product_tear      120469 non-null  object \n",
      " 13  product_tonality  120469 non-null  object \n",
      " 14  product_gender    120469 non-null  object \n",
      " 15  product_age       120469 non-null  object \n",
      " 16  product_category  120469 non-null  object \n",
      " 17  product_fit       120469 non-null  object \n",
      " 18  product_rise      120469 non-null  object \n",
      " 19  product_neckline  120469 non-null  object \n",
      " 20  product_sleeve    120469 non-null  object \n",
      " 21  product_denim     120469 non-null  object \n",
      " 22  product_stretch   120469 non-null  object \n",
      " 23  product_wash      120469 non-null  object \n",
      "dtypes: float64(7), object(17)\n",
      "memory usage: 23.0+ MB\n"
     ]
    }
   ],
   "source": [
    "masterdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d448a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_dict = masterdf.groupby([\n",
    "                                            'user_id',\n",
    "                                            'user_gender',\n",
    "                                            'user_age',\n",
    "                                            'user_lat',\n",
    "                                            'user_long',\n",
    "                                            'timestamp',\n",
    "                                            'product_id',\n",
    "                                            'price',\n",
    "                                            'product_colour',\n",
    "                                            'product_tear',\n",
    "                                            'product_tonality',\n",
    "                                            'product_gender',\n",
    "                                            'product_age',\n",
    "                                            'product_category',\n",
    "                                            'product_fit',\n",
    "                                            'product_rise',\n",
    "                                            'product_neckline',\n",
    "                                            'product_sleeve',\n",
    "                                            'product_denim',\n",
    "                                            'product_stretch',\n",
    "                                            'product_wash',\n",
    "                                            'user_rating'\n",
    "    \n",
    "])['quantity'].sum().reset_index()\n",
    "\n",
    "purchases_dict = {name: np.array(value) for name, value in purchases_dict.items()}\n",
    "purchases = tf.data.Dataset.from_tensor_slices(purchases_dict)\n",
    "\n",
    "products_dict = masterdf[['product_id']].drop_duplicates().dropna()\n",
    "products_dict = {name: np.array(value) for name, value in products_dict.items()}\n",
    "products = tf.data.Dataset.from_tensor_slices(products_dict)\n",
    "\n",
    "purchases = purchases.map(lambda x: {\n",
    "                                            'user_id' : x['user_id'], \n",
    "                                            'user_gender' : x['user_gender'],\n",
    "                                            'user_age' : x['user_age'],\n",
    "                                            'user_lat' : x['user_lat'],\n",
    "                                            'user_long' : x['user_long'],   \n",
    "                                            'product_id' : x['product_id'],\n",
    "                                            'quantity': x['quantity'],\n",
    "                                            'price' : x['price'],\n",
    "                                            'timestamp': x['timestamp'],\n",
    "                                            'product_colour': x['product_colour'],\n",
    "                                            'product_tear' : x['product_tear'], \n",
    "                                            'product_tonality' : x['product_tonality'],\n",
    "                                            'product_gender': x['product_gender'],\n",
    "                                            'product_age' : x['product_age'], \n",
    "                                            'product_category' : x['product_category'],\n",
    "                                            'product_fit': x['product_fit'],\n",
    "                                            'product_rise' : x['product_rise'], \n",
    "                                            'product_neckline' : x['product_neckline'],\n",
    "                                            'product_sleeve': x['product_sleeve'],\n",
    "                                            'product_denim' : x['product_denim'], \n",
    "                                            'product_stretch' : x['product_stretch'],\n",
    "                                            'product_wash': x['product_wash'],\n",
    "                                            'user_rating': x['user_rating']\n",
    "})\n",
    "\n",
    "products = products.map(lambda x: x['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "10f8b2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'00501-0101', b'00501-0114', b'00501-0162', b'00501-0165',\n",
       "       b'00501-2861', b'00501-3058', b'00501-3061', b'00501-3107',\n",
       "       b'00501-3132', b'00501-3139'], dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_ids = products.batch(1_000)\n",
    "user_ids = purchases.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_product_ids = np.unique(np.concatenate(list(product_ids)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "33799294",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_gender = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"user_gender\"]))))\n",
    "unique_product_colour = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_colour\"]))))\n",
    "unique_product_tear = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_tear\"]))))\n",
    "unique_product_tonality = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_tonality\"]))))\n",
    "unique_product_gender = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_gender\"]))))\n",
    "unique_product_age = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_age\"]))))\n",
    "unique_product_category = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_category\"]))))\n",
    "unique_product_fit = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_fit\"]))))\n",
    "unique_product_rise = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_rise\"]))))\n",
    "unique_product_neckline = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_neckline\"]))))\n",
    "unique_product_sleeve = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_sleeve\"]))))\n",
    "unique_product_denim = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_denim\"]))))\n",
    "unique_product_stretch = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_stretch\"]))))\n",
    "unique_product_wash = np.unique(np.concatenate(list(purchases.batch(1_000_000).map(lambda x: x[\"product_wash\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a86853a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = list(purchases.map(lambda x: x[\"timestamp\"]).batch(100))\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    np.concatenate(list(purchases.map(lambda x: x[\"timestamp\"]).batch(100))).min(), \n",
    "    np.concatenate(list(purchases.map(lambda x: x[\"timestamp\"]).batch(100))).max(), num=1000,\n",
    ")\n",
    "\n",
    "user_age = list(purchases.map(lambda x: x[\"user_age\"]).batch(100))\n",
    "\n",
    "user_age = list(purchases.map(lambda x: x[\"user_age\"]).batch(100))\n",
    "\n",
    "user_age_buckets = np.linspace(\n",
    "    np.concatenate(user_age).min(), \n",
    "    np.concatenate(user_age).max(), num=1000,\n",
    ")\n",
    "\n",
    "latitude = list(purchases.map(lambda x: x[\"user_lat\"]).batch(100))\n",
    "\n",
    "user_lat_buckets = np.linspace(\n",
    "    np.concatenate(latitude).min(), \n",
    "    np.concatenate(latitude).max(), num=1000,\n",
    ")\n",
    "\n",
    "longitude = list(purchases.map(lambda x: x[\"user_long\"]).batch(100))\n",
    "\n",
    "user_long_buckets = np.linspace(\n",
    "    np.concatenate(longitude).min(), \n",
    "    np.concatenate(longitude).max(), num=1000,\n",
    ")\n",
    "\n",
    "price = list(purchases.map(lambda x: x[\"price\"]).batch(100))\n",
    "\n",
    "price_buckets = np.linspace(\n",
    "    np.concatenate(list(purchases.map(lambda x: x[\"price\"]).batch(100))).min(), \n",
    "    np.concatenate(list(purchases.map(lambda x: x[\"price\"]).batch(100))).max(), num=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ed23de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "        self.gender_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_gender, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_gender) + 1, 32),\n",
    "        ])\n",
    "        #self.timestamp_embedding = tf.keras.Sequential([\n",
    "        #    tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "        #    tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "        #])\n",
    "        #self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "        #self.normalized_timestamp.adapt(timestamps)\n",
    "        \n",
    "        self.age_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(user_age_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(user_age_buckets) + 1, 32),\n",
    "        ])\n",
    "        self.normalized_age = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "        self.normalized_age.adapt(user_age_buckets)\n",
    "        \n",
    "        self.lat_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(user_lat_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(user_lat_buckets) + 1, 32),\n",
    "        ])\n",
    "        self.normalized_lat = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "        self.normalized_lat.adapt(user_lat_buckets)\n",
    "        \n",
    "        self.long_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(user_long_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(user_long_buckets) + 1, 32),\n",
    "        ])\n",
    "        self.normalized_long = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "        self.normalized_long.adapt(user_lat_buckets)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Take the input dictionary, pass it through each input layer,\n",
    "        # and concatenate the result.\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.gender_embedding(inputs[\"user_gender\"]),\n",
    "            #self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "            #self.normalized_timestamp(inputs[\"timestamp\"]),\n",
    "            self.age_embedding(inputs[\"user_age\"]),\n",
    "            self.normalized_age(inputs[\"user_age\"]),\n",
    "            self.lat_embedding(inputs[\"user_lat\"]),\n",
    "            self.normalized_lat(inputs[\"user_lat\"]),\n",
    "            self.long_embedding(inputs[\"user_long\"]),\n",
    "            self.normalized_long(inputs[\"user_long\"]),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cb36cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "    \"\"\"Model for encoding user queries.\"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"Model for encoding user queries.\n",
    "\n",
    "        Args:\n",
    "          layer_sizes:\n",
    "            A list of integers where the i-th entry represents the number of units\n",
    "            the i-th layer contains.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # We first use the user model for generating embeddings.\n",
    "        self.embedding_model = UserModel()\n",
    "\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "        # No activation for the last layer.\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        feature_embedding = self.embedding_model(inputs)\n",
    "        return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ae313b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        self.product_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_ids,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_ids) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_text_embedding = tf.keras.Sequential([\n",
    "          self.product_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_vectorizer.adapt(products)\n",
    "        \n",
    "        self.product_colour_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_colour,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_colour) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_colour_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_colour_text_embedding = tf.keras.Sequential([\n",
    "          self.product_colour_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_colour_vectorizer.adapt(purchases['product_colour'])\n",
    "        \n",
    "        self.product_tear_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_tear,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_tear) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_tear_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_tear_text_embedding = tf.keras.Sequential([\n",
    "          self.product_tear_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_tear_vectorizer.adapt(purchases['product_tear'])\n",
    "        \n",
    "        self.product_tonality_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_tonality,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_tonality) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_tonality_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_tonality_text_embedding = tf.keras.Sequential([\n",
    "          self.product_tonality_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_tonality_vectorizer.adapt(purchases['product_tonality'])\n",
    "        \n",
    "        self.product_gender_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_gender,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_gender) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_gender_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_gender_text_embedding = tf.keras.Sequential([\n",
    "          self.product_gender_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_gender_vectorizer.adapt(purchases['product_gender'])\n",
    "        \n",
    "        self.product_age_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_age,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_age) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_age_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_age_text_embedding = tf.keras.Sequential([\n",
    "          self.product_age_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_age_vectorizer.adapt(purchases['product_age'])\n",
    "        \n",
    "        self.product_category_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_category,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_category) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_category_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_category_text_embedding = tf.keras.Sequential([\n",
    "          self.product_category_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAvercategoryPooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_category_vectorizer.adapt(purchases['product_category'])\n",
    "        \n",
    "        self.product_fit_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_fit,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_fit) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_fit_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_fit_text_embedding = tf.keras.Sequential([\n",
    "          self.product_fit_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAverfitPooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_fit_vectorizer.adapt(purchases['product_fit'])\n",
    "        \n",
    "        self.product_rise_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_rise,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_rise) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_rise_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_rise_text_embedding = tf.keras.Sequential([\n",
    "          self.product_rise_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAverrisePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_rise_vectorizer.adapt(purchases['product_rise'])\n",
    "        \n",
    "        self.product_neckline_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_neckline,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_neckline) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_neckline_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_neckline_text_embedding = tf.keras.Sequential([\n",
    "          self.product_neckline_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAvernecklinePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_neckline_vectorizer.adapt(purchases['product_neckline'])\n",
    "        \n",
    "        self.product_sleeve_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_sleeve,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_sleeve) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_sleeve_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_sleeve_text_embedding = tf.keras.Sequential([\n",
    "          self.product_sleeve_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAversleevePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_sleeve_vectorizer.adapt(purchases['product_sleeve'])\n",
    "        \n",
    "        self.product_denim_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_denim,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_denim) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_denim_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_denim_text_embedding = tf.keras.Sequential([\n",
    "          self.product_denim_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAverdenimPooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_denim_vectorizer.adapt(purchases['product_denim'])\n",
    "        \n",
    "        self.product_stretch_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_stretch,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_stretch) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_stretch_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_stretch_text_embedding = tf.keras.Sequential([\n",
    "          self.product_stretch_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAverstretchPooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_stretch_vectorizer.adapt(purchases['product_stretch'])\n",
    "        \n",
    "        self.product_wash_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_product_wash,mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_product_wash) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.product_wash_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.product_wash_text_embedding = tf.keras.Sequential([\n",
    "          self.product_wash_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAverwashPooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.product_wash_vectorizer.adapt(purchases['product_wash'])\n",
    "        \n",
    "        self.price_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(price_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(price_buckets) + 1, 32),\n",
    "        ])\n",
    "        self.normalized_price = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "        self.normalized_age.adapt(price_buckets)\n",
    "\n",
    "    def call(self, products):\n",
    "        return tf.concat([\n",
    "            self.product_embedding(inputs['product_id']),\n",
    "            self.product_text_embedding(inputs['product_id']),\n",
    "            self.product_colour_embedding(inputs['product_colour']),\n",
    "            self.product_colour_text_embedding(inputs['product_colour']),\n",
    "            self.product_tear_embedding(inputs['product_tear']),\n",
    "            self.product_tear_text_embedding(inputs['product_tear']),\n",
    "            self.product_tonality_embedding(inputs['product_tonality']),\n",
    "            self.product_tonality_text_embedding(inputs['product_tonality']),\n",
    "            self.product_gender_embedding(inputs['product_gender']),\n",
    "            self.product_gender_text_embedding(inputs['product_gender']),\n",
    "            self.product_age_embedding(inputs['product_age']),\n",
    "            self.product_age_text_embedding(inputs['product_age']),\n",
    "            self.product_category_embedding(inputs['product_category']),\n",
    "            self.product_category_text_embedding(inputs['product_category']),\n",
    "            self.product_category_embedding(inputs['product_category']),\n",
    "            self.product_category_text_embedding(inputs['product_category']),\n",
    "            self.product_fit_embedding(inputs['product_fit']),\n",
    "            self.product_fit_text_embedding(inputs['product_fit']),\n",
    "            self.product_rise_embedding(inputs['product_rise']),\n",
    "            self.product_rise_text_embedding(inputs['product_rise']),\n",
    "            self.product_neckline_embedding(inputs['product_neckline']),\n",
    "            self.product_neckline_text_embedding(inputs['product_neckline']),\n",
    "            self.product_sleeve_embedding(inputs['product_sleeve']),\n",
    "            self.product_sleeve_text_embedding(inputs['product_sleeve']),\n",
    "            self.product_denim_embedding(inputs['product_denim']),\n",
    "            self.product_denim_text_embedding(inputs['product_denim']),\n",
    "            self.product_stretch_embedding(inputs['product_stretch']),\n",
    "            self.product_stretch_text_embedding(inputs['product_stretch']),\n",
    "            self.product_wash_embedding(inputs['product_wash']),\n",
    "            self.product_wash_text_embedding(inputs['product_wash']),\n",
    "            self.price_embedding(inputs[\"price\"]),\n",
    "            self.normalized_price(inputs[\"price\"]),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db40ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateModel(tf.keras.Model):\n",
    "    \"\"\"Model for encoding movies.\"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"Model for encoding movies.\n",
    "\n",
    "        Args:\n",
    "          layer_sizes:\n",
    "            A list of integers where the i-th entry represents the number of units\n",
    "            the i-th layer contains.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_model = ProductModel()\n",
    "\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "        # No activation for the last layer.\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        feature_embedding = self.embedding_model(inputs)\n",
    "        return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bf909146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel(layer_sizes)\n",
    "        self.candidate_model = CandidateModel(layer_sizes)\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=movies.batch(128).map(self.candidate_model),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        def compute_loss(self, features, training=False):\n",
    "        # We only pass the user id and timestamp features into the query model. This\n",
    "        # is to ensure that the training inputs would have the same keys as the\n",
    "        # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "        # error when loading the query model after saving it.\n",
    "            query_embeddings = self.query_model({\n",
    "                \"user_id\": features[\"user_id\"],\n",
    "                #\"timestamp\": features[\"timestamp\"],\n",
    "                \"user_age\": features[\"user_age\"],\n",
    "                \"user_gender\": features[\"user_gender\"],\n",
    "                \"user_lat\": features[\"user_lat\"],\n",
    "                \"user_long\": features[\"user_long\"]\n",
    "            })\n",
    "            product_embeddings = self.candidate_model({\n",
    "                \"product_id\": features[\"product_id\"],\n",
    "                \"product_colour\": features[\"product_colour\"],\n",
    "                \"product_tear\": features[\"product_tear\"],\n",
    "                \"product_tonality\": features[\"product_tonality\"],\n",
    "                \"product_gender\": features[\"product_gender\"],\n",
    "                \"product_age\": features[\"product_age\"],\n",
    "                \"product_category\": features[\"product_category\"],\n",
    "                \"product_fit\": features[\"product_fit\"],\n",
    "                \"product_rise\": features[\"product_rise\"],\n",
    "                \"product_neckline\": features[\"product_neckline\"],\n",
    "                \"product_sleeve\": features[\"product_sleeve\"],\n",
    "                \"product_denim\": features[\"product_denim\"],\n",
    "                \"product_stretch\": features[\"product_stretch\"],\n",
    "                \"product_wash\": features[\"product_wash\"],\n",
    "                \"price\":features[\"price\"]\n",
    "            })\n",
    "\n",
    "            return self.task(\n",
    "                query_embeddings, product_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "947c19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle data and split between train and test.\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "shuffled = purchases.shuffle(len(masterdf), seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(round(len(masterdf)*0.8))\n",
    "test = shuffled.skip(round(len(masterdf)*0.8)).take(round(len(masterdf)*0.2))\n",
    "\n",
    "cached_train = train.shuffle(len(masterdf)).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "43e00f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\joaoa\\anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:121 adapt_step  *\n        self._adapt_maybe_build(data)\n    C:\\Users\\joaoa\\anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:284 _adapt_maybe_build  **\n        self.build(data_shape)\n    C:\\Users\\joaoa\\anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py:145 build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_66552/363362480.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;31m# Less epoch to get to predictions quicker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_66552/2223560086.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layer_sizes)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQueryModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcandidate_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCandidateModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         self.task = tfrs.tasks.Retrieval(\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_66552/663102029.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layer_sizes)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# We first use the user model for generating embeddings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUserModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Then construct the layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_66552/2129607900.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalized_age\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalized_age\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_age_buckets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         self.lat_embedding = tf.keras.Sequential([\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    246\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 759\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    760\u001b[0m             *args, **kwds))\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSys\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\joaoa\\anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:121 adapt_step  *\n        self._adapt_maybe_build(data)\n    C:\\Users\\joaoa\\anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:284 _adapt_maybe_build  **\n        self.build(data_shape)\n    C:\\Users\\joaoa\\anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py:145 build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None], with unknown axis at index: 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3 # Less epoch to get to predictions quicker\n",
    "\n",
    "model = MainModel([32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=5,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0)\n",
    "\n",
    "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db4999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
